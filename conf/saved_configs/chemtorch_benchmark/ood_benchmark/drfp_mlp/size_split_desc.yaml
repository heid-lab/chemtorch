ckpt_path: null
data_module:
  _target_: chemtorch.core.data_module.DataModule
  _convert_: object
  cache: false
  max_cache_size: null
  data_pipeline:
    _target_: chemtorch.components.data_pipeline.SimpleDataPipeline
    column_mapper:
      _target_: chemtorch.components.data_pipeline.column_mapper.ColumnFilterAndRename
      label: dE0
      smiles: smiles
    data_source:
      _target_: chemtorch.components.data_pipeline.data_source.SingleCSVSource
      data_path: data/rdb7/barriers/forward/data.csv
    data_splitter:
      _target_: chemtorch.components.data_pipeline.data_splitter.SizeSplitter
      train_ratio: 0.8
      val_ratio: 0.1
      test_ratio: 0.1
      sort_order: descending
      save_path: null
  dataloader_factory:
    _partial_: true
    _target_: torch.utils.data.DataLoader
    batch_size: 64
    num_workers: 3
    pin_memory: true
  precompute_all: true
  representation:
    _recursive_: false
    _target_: chemtorch.components.representation.fingerprint.drfp.DRFP
    include_hydrogens: false
    min_radius: 0
    n_folded_length: 2048
    radius: 3
    rings: true
    root_central_atom: true
  subsample: null
fp_length: 2048
group_name: chemtorch_data_split_benchmark
load_model: false
log: true
model:
  _target_: chemtorch.components.model.mlp.MLP
  act: relu
  dropout: 0.015217512179126504
  hidden_size: 256
  in_channels: 2048
  norm: None
  num_hidden_layers: 2
  out_channels: 1
model.dropout: 0.015217512179126504
model.hidden_size: 256
model.norm: null
model.num_hidden_layers: 2
name: drfp_mlp
parameter_limit: null
predictions_save_path: null
predictions_save_dir: predictions/chemtorch_paper/rdb7_data_split_benchmark/drfp_mlp/size_desc/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}
project_name: chemtorch
routine:
  _convert_: object
  _partial_: true
  _target_: chemtorch.core.routine.RegressionRoutine
  loss:
    _target_: torch.nn.modules.loss.MSELoss
  lr_scheduler:
    frequency: 1
    interval: epoch
    scheduler:
      _partial_: true
      _target_: chemtorch.core.scheduler.cosine_with_warmup_lr.CosineWithWarmupLR
      end_factor: 1.0
      eta_min: 0.0
      num_training_steps: 100
      num_warmup_steps: 10
      start_factor: 1.0e-06
  metrics:
    test:
      _target_: torchmetrics.MetricCollection
      metrics:
        mae:
          _target_: torchmetrics.MeanAbsoluteError
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    train:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    val:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
  optimizer:
    _partial_: true
    _target_: torch.optim.AdamW
    amsgrad: false
    betas:
    - 0.9
    - 0.999
    capturable: false
    eps: 1.0e-08
    foreach: null
    lr: 0.006404259921038013
    maximize: false
    weight_decay: 2.04461442479684e-05
  standardizer:
    _target_: chemtorch.utils.Standardizer
    mean: 80.0102253144654
    std: 21.684054853890036
routine.optimizer.lr: 0.006404259921038013
routine.optimizer.weight_decay: 2.04461442479684e-05
run_name: size_desc_drfp_mlp
save_predictions_for:
- train
- test
seed: 0
tasks:
- fit
- test
train_label_mean: 80.0102253144654
train_label_std: 21.684054853890036
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    min_delta: 0.01
    mode: min
    monitor: val_loss_epoch
    patience: 30
  default_root_dir: ./lightning_logs
  gradient_clip_val: 1.0
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
  max_epochs: 100
  enable_checkpointing: false
  checkpoint_callback:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${trainer.default_root_dir}/chemtorch_paper/rdb7_data_split_benchmark/size_desc/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}/checkpoints
    monitor: val_loss
    save_top_k: 1
    save_last: true
hydra:
  output_subdir: null
  run:
    dir: .
