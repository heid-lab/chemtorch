ckpt_path: null
data_module:
  _target_: chemtorch.core.data_module.DataModule
  _convert_: object
  cache: false
  max_cache_size: null
  data_pipeline:
    _target_: chemtorch.components.data_pipeline.SimpleDataPipeline
    column_mapper:
      _target_: chemtorch.components.data_pipeline.column_mapper.ColumnFilterAndRename
      label: dE0
      smiles: smiles
    data_source:
      _target_: chemtorch.components.data_pipeline.data_source.SingleCSVSource
      data_path: data/rdb7/barriers/forward/data.csv
    data_splitter:
      _target_: chemtorch.components.data_pipeline.data_splitter.TargetSplitter
      train_ratio: 0.8
      val_ratio: 0.1
      test_ratio: 0.1
      sort_order: ascending
      save_path: null
  dataloader_factory:
    _partial_: true
    _target_: torch.utils.data.DataLoader
    batch_size: 64
    num_workers: 3
    pin_memory: true
  representation:
    _target_: chemtorch.components.representation.token.token_representation_base.TokenRepresentationBase
    max_sentence_length: 400
    pad_token: '[PAD]'
    tokenizer:
      _target_: chemtorch.components.representation.token.tokenizer.reaction_tokenizer.ReactionTokenizer
      molecule_tokenizer:
        _target_: chemtorch.components.representation.token.tokenizer.molecule_tokenizer.smiles_symbol_tokenizer.SmilesSymbolTokenizer
        vocab_path: resources/base_vocab.txt
        pad_token: '[PAD]'
        unk_token: '[UNK]'
    unk_token: '[UNK]'
  precompute_all: true
  subsample: null
group_name: chemtorch_data_split_benchmark
load_model: false
log: true
model:
  _target_: chemtorch.components.model.han.HAN
  class_num: 1
  dropout: 0.1361373846961515
  embedding_hidden_channels: 256
  embedding_in_channels: 604
  gru_hidden_channels: 256
model.dropout: 0.1361373846961515
model.embedding_hidden_channels: 256
model.gru_hidden_channels: 256
name: atom_han
num_classes: 1
parameter_limit: null
predictions_save_path: null
predictions_save_dir: predictions/chemtorch_paper/rdb7_data_split_benchmark/atom_han/target_asc/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}
project_name: chemtorch
routine:
  _convert_: object
  _partial_: true
  _target_: chemtorch.core.routine.RegressionRoutine
  loss:
    _target_: torch.nn.modules.loss.MSELoss
  lr_scheduler:
    frequency: 1
    interval: epoch
    scheduler:
      _partial_: true
      _target_: chemtorch.core.scheduler.cosine_with_warmup_lr.CosineWithWarmupLR
      end_factor: 1.0
      eta_min: 0.0
      num_training_steps: 100
      num_warmup_steps: 10
      start_factor: 1.0e-06
  metrics:
    test:
      _target_: torchmetrics.MetricCollection
      metrics:
        mae:
          _target_: torchmetrics.MeanAbsoluteError
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    train:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    val:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
  optimizer:
    _partial_: true
    _target_: torch.optim.AdamW
    amsgrad: false
    betas:
    - 0.9
    - 0.999
    capturable: false
    eps: 1.0e-08
    foreach: null
    lr: 0.0006884302350655184
    maximize: false
    weight_decay: 0.0023109641884162787
  standardizer:
    _target_: chemtorch.utils.Standardizer
    mean: 80.0102253144654
    std: 21.684054853890036
routine.optimizer.lr: 0.0006884302350655184
routine.optimizer.weight_decay: 0.0023109641884162787
run_name: target_asc_atom_han
save_predictions_for:
- train
- test
seed: 0
tasks:
- fit
- test
train_label_mean: 80.0102253144654
train_label_std: 21.684054853890036
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    min_delta: 0.01
    mode: min
    monitor: val_loss_epoch
    patience: 30
  default_root_dir: ./lightning_logs
  gradient_clip_val: 1.0
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
  max_epochs: 100
  enable_checkpointing: false
  checkpoint_callback:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${trainer.default_root_dir}/chemtorch_paper/rdb7_data_split_benchmark/target_asc/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}/checkpoints
    monitor: val_loss
    save_top_k: 1
    save_last: true
vocab_size: 604
hydra:
  output_subdir: null
  run:
    dir: .
