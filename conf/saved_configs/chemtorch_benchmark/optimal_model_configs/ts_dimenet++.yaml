ckpt_path: null
data_pipeline:
  _target_: chemtorch.components.data_pipeline.SimpleDataPipeline
  column_mapper:
    _target_: chemtorch.components.data_pipeline.column_mapper.ColumnFilterAndRename
    label: dE0
    reaction_dir: rxn
    smiles: smiles
  data_source:
    _target_: chemtorch.components.data_pipeline.data_source.SingleCSVSource
    data_path: data/rdb7/barriers/forward/data.csv
  data_splitter:
    _target_: chemtorch.components.data_pipeline.data_splitter.RatioSplitter
    save_path: null
    test_ratio: 0.1
    train_ratio: 0.8
    val_ratio: 0.1
dataloader:
  _partial_: true
  _target_: torch_geometric.loader.DataLoader
  batch_size: 32
  num_workers: 3
  pin_memory: true
dataloader.batch_size: 32
dataset:
  _partial_: true
  _target_: chemtorch.components.dataset.GraphDataset
  cache: true
  max_cache_size: null
  precompute_all: true
  representation:
    _target_: chemtorch.components.representation.graph.ts_3d_graph.TS3DGraph
    root_dir: data/rdb7/geometries/forward
  subsample: null
group_name: chemtorch_benchmark/optimal_model_configs
load_model: false
log: true
mean: 80.0102253144654
model:
  _target_: chemtorch.components.model.dimenetplusplus.DimeNetPlusPlus
  act: swish
  basis_emb_size: 8
  cutoff: 5.635654239463599
  envelope_exponent: 5
  head:
    _target_: chemtorch.components.model.mlp.MLP
    act: relu
    dropout: 0.02
    hidden_size: 256
    hidden_sizey: 128
    in_channels: 128
    num_hidden_layers: 1
    out_channels: 1
  hidden_channels: 256
  int_emb_size: 64
  max_num_neighbors: 32
  num_after_skip: 2
  num_before_skip: 1
  num_blocks: 6
  num_output_layers: 3
  num_radial: 6
  num_spherical: 7
  out_channels: 128
  out_emb_channels: 256
  output_initializer: zeros
model.cutoff: 5.635654239463599
model.hidden_channels: 256
model.num_blocks: 6
name: ts_dimenet++
parameter_limit: null
prediction_save_path: null
predictions_save_dir: predictions/chemtorch_paper/rdb7_model_benchmark/ts_dimenet++/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}
save_predictions_for: [train, test]
project_name: chemtorch
routine:
  _convert_: object
  _partial_: true
  _target_: chemtorch.core.routine.RegressionRoutine
  loss:
    _target_: torch.nn.modules.loss.MSELoss
  lr_scheduler:
    frequency: 1
    interval: epoch
    scheduler:
      _partial_: true
      _target_: chemtorch.core.scheduler.cosine_with_warmup_lr.CosineWithWarmupLR
      end_factor: 1.0
      eta_min: 0.0
      num_training_steps: 100
      num_warmup_steps: 10
      start_factor: 1.0e-06
  metrics:
    test:
      _target_: torchmetrics.MetricCollection
      metrics:
        mae:
          _target_: torchmetrics.MeanAbsoluteError
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    train:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
    val:
      _target_: torchmetrics.MetricCollection
      metrics:
        rmse:
          _target_: torchmetrics.MeanSquaredError
          squared: false
  optimizer:
    _partial_: true
    _target_: torch.optim.AdamW
    amsgrad: false
    betas:
    - 0.9
    - 0.999
    capturable: false
    eps: 1.0e-08
    foreach: null
    lr: 0.0001210446176830952
    maximize: false
    weight_decay: 4.540637426271681e-06
  standardizer:
    _target_: chemtorch.utils.Standardizer
    mean: 80.0102253144654
    std: 21.684054853890036
routine.optimizer.lr: 0.0001210446176830952
routine.optimizer.weight_decay: 4.540637426271681e-06
run_name: null
runtime_args_from_dataset:
- mean
- std
seed: 0
std: 21.684054853890036
tasks:
- fit
- test
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    min_delta: 0.01
    mode: min
    monitor: val_loss_epoch
    patience: 30
  checkpoint_callback:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${trainer.default_root_dir}/chemtorch_paper/rdb7_model_benchmark/ts_dimenet++/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}/checkpoints
    monitor: val_loss
    save_last: true
    save_top_k: 1
  default_root_dir: ./lightning_logs
  enable_checkpointing: false
  gradient_clip_val: 1.0
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
  max_epochs: 100
