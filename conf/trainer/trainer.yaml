_target_: lightning.Trainer

# CONTROLS
default_root_dir: ./lightning_logs
enable_checkpointing: false
max_epochs: -1  # set to -1 for no limit
accelerator: auto
gradient_clip_val: 1.0

# LOGGING
logger: 
  _target_: lightning.pytorch.loggers.WandbLogger
  # log_model: true

# CALLBACKS
callbacks:
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val_loss_epoch
    min_delta: 0.01
    patience: 30
    mode: min

# If enable_checkpointing is true, this callback will be added to the callbacks list at runtime
checkpoint_callback:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  # Checkpoints are saved to lightning_logs/<data_pipeline>/<model>/seed_<seed>_<timestamp>/checkpoints by default
  # Users should override this with a more meaningful path for their use case
  dirpath: ${trainer.default_root_dir}/${hydra:runtime.choices.data_pipeline}/${hydra:runtime.choices.model}/seed_${seed}_${now:%Y-%m-%d_%H-%M-%S}/checkpoints
  monitor: val_loss_epoch
  save_top_k: 1
  save_last: true


